{
    "id": 71,
    "name": "blog/graph",
    "createdAt": "2026-01-15T13:00:46.903Z",
    "updatedAt": "2026-01-15T13:00:46.903Z",
    "type": "default",
    "isDeleted": false,
    "versionOnTranslationId": null,
    "searchCategorySlug": "blog",
    "regions": [],
    "pageId": 71,
    "regionCode": "en",
    "publishedVersionId": 216,
    "lastVersionId": 216,
    "content": "blocks:\n  - type: blog-header-block\n    resetPaddings: true\n    paddingBottom: l\n    width: m\n    verticalOffset: m\n    theme: dark\n    background:\n      image:\n        src: >-\n          https://storage.yandexcloud.net/gravity-landing-static/blog/blog-cover-bg.png\n        disableCompress: true\n      color: '#CCDAFF'\n      fullWidth: false\n  - type: blog-layout-block\n    resetPaddings: true\n    mobileOrder: reverse\n    children:\n      - type: blog-yfm-block\n        resetPaddings: true\n        column: right\n        text: >\n\n          ![image](https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/speaker.jpg\n          =80x)\n\n\n          **Andrey Shchetinin**\n\n          Senior Frontend Developer\n      - type: blog-yfm-block\n        column: right\n        resetPaddings: true\n        text: |\n\n          In this article:\n\n            - [Where the task came from](#task)\n            - [How we arrived at the solution](#solution)\n            - [Customization](#customization)\n            - [Our graph library: what the benefits are and how to use it](#library)\n            - [Are there any alternatives?](#analogs)\n            - [Plans for the future](#future)\n            - [Try it and join in](#try)\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          Hi! My name is Andrey, I’m a frontend developer on the User Experience\n          team for Yandex infrastructure services. We develop Gravity UI—an\n          open-source design system and React component library used by dozens\n          of products inside the company and beyond. Today I’ll расскажу how we\n          ran into the task of visualizing complex graphs, why existing\n          solutions didn’t work for us, and how @gravity‑ui/graph ended up being\n          created—a library we decided to open to the community.\n\n\n          This story started with a practical problem: we needed to render\n          graphs with 10,000+ elements and interactive components. At Yandex\n          there are many projects where users build complex data processing\n          pipelines—from simple ETL processes to machine learning. When such\n          pipelines are created programmatically, the number of blocks can reach\n          tens of thousands.\n\n\n          Existing solutions didn’t work for us:\n\n            * **HTML/SVG libraries** look great and are convenient to develop with, but they start lagging already at hundreds of elements.\n            * **Canvas solutions** handle performance, but require a huge amount of code to build complex UI elements.\n\n          Drawing a button with rounded corners and a gradient in Canvas isn’t\n          hard. However, problems appear when you need to create complex custom\n          controls or layout—you’ll have to write dozens of lines of low-level\n          drawing commands. Each UI element has to be programmed from\n          scratch—from click handling to animations. And we needed full-fledged\n          UI components: buttons, selects, input fields, drag-and-drop.\n\n\n          We decided not to choose between Canvas and HTML, but to use the best\n          of both technologies. The idea was simple: automatically switch\n          between modes depending on how close the user is looking at the graph.\n      - type: blog-media-block\n        column: left\n        resetPaddings: true\n        paddingBottom: s\n        text: ''\n        image:\n          src: >-\n            https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic1.png\n        fullscreen: true\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: |\n\n          #### Try it yourself\n\n            * [GitHub repository](https://github.com/gravity-ui/graph){target=\"_blank\"}\n            * [Storybook with examples](https://preview.gravity-ui.com/graph/){target=\"_blank\"}\n            * [Playground](https://gravity-ui.com/ru/libraries/graph/playground){target=\"_blank\"}\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          ## Where the task came from{#task}\n\n\n          ### Nirvana and its graphs\n\n\n          At Yandex we have a service called Nirvana for creating and running\n          data-processing graphs (we wrote about it\n          [here](https://habr.com/ru/companies/yandex/articles/351016/){target=\"_blank\"}\n          back in 2018). It’s a large, popular service that has been around for\n          a long time.\n\n\n          Some users build graphs by hand—dragging with the mouse, adding\n          blocks, connecting them. With those graphs there’s no problem: there\n          aren’t many blocks, and everything works great. But there are projects\n          that create graphs programmatically. And that’s where the difficulties\n          start: they can put up to 10,000 operations into a single graph. And\n          you end up with this:\n      - type: blog-media-block\n        column: left\n        resetPaddings: true\n        text: ''\n        image:\n          src: >-\n            https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic2.png\n        fullscreen: true\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          {% cut \"And this:\" %}\n\n\n          ![image](https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic2-1.png\n          =830x)\n\n          ![image](https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic2-2.png\n          =830x)\n\n          ![image](https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic2-3.png\n          =830x)\n\n          ![image](https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic2-4.png\n          =830x)\n\n          ![image](https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic2-5.png\n          =830x)\n\n\n          {% endcut %}\n\n\n          A typical HTML + SVG combo simply can’t handle such graphs. The\n          browser starts lagging, memory leaks, the user suffers. We tried to\n          solve the problem head-on: optimize HTML rendering, but sooner or\n          later we hit physical limits—DOM is simply not designed for thousands\n          of simultaneously visible floating interactive elements.\n\n\n          We needed a different solution, and in the browser we only had Canvas\n          left. Only it can provide the required performance.\n\n\n          The first thought was to find a ready-made solution. It was 2017–2018,\n          and we went through popular libraries for Canvas or graph rendering,\n          but all solutions ran into the same problem: either use Canvas with\n          primitive elements, or use HTML/SVG and sacrifice performance.\n\n\n          What if we don’t choose?\n\n\n          ### Level of Details: inspiration from GameDev\n\n\n          In GameDev and cartography there’s a great concept—Level of Details\n          (LOD). This technique was born out of necessity—how do you show a huge\n          world without killing performance?\n\n\n          The idea is simple: a single object can have several levels of detail\n          depending on how closely it’s viewed. In games it’s especially\n          noticeable:\n\n            * Far away you see mountains—simple polygons with a basic texture.\n            * As you get closer—details appear: grass, rocks, shadows.\n            * Even closer—you can see individual leaves on trees.\n\n          Nobody renders millions of grass polygons when the player is standing\n          on a mountain peak looking into the distance.\n\n\n          In maps, the principle is the same—each zoom level has its own dataset\n          and its own detail level:\n\n            * Continent scale—only countries are visible.\n            * Zooming into a city—streets and districts appear.\n            * Even closer—house numbers, cafés, bus stops.\n\n          We realized: the user doesn’t need interactive buttons at a\n          large-scale view of a graph with 10,000 blocks—they won’t see them\n          anyway and won’t be able to work with them.\n\n\n          Moreover, attempting to render 10,000 HTML elements at once will\n          freeze the browser. But when the user zooms into a specific area, the\n          number of visible blocks drops sharply—from 10,000 to, say, 50. That’s\n          when resources are freed up for HTML components with rich\n          interactivity.\n\n\n          ### Three levels in our Level of Details scheme\n\n\n          #### Minimalistic (scale 0.1–0.3) — Canvas with simple primitives\n\n\n          In this mode the user sees the overall architecture of the system:\n          where the main groups of blocks are located and how they are\n          connected. Each block is a simple rectangle with basic color coding.\n          No text, buttons, or detailed icons. But you can comfortably render\n          thousands of elements. At this level the user selects an area for\n          detailed exploration.\n      - type: blog-media-block\n        column: left\n        resetPaddings: true\n        paddingBottom: s\n        text: ''\n        image:\n          src: >-\n            https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic3.png\n        fullscreen: true\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          #### Schematic (scale 0.3–0.7) — Canvas with details\n\n\n          Block names, status icons, and connection anchors appear. Text is\n          rendered via the Canvas API—this is fast, but styling options are\n          limited. Connections between blocks become more informative: you can\n          show the direction of the data flow, the connection status. This is a\n          transitional mode where Canvas performance is combined with basic\n          informativeness.\n      - type: blog-media-block\n        column: left\n        resetPaddings: true\n        paddingBottom: s\n        text: ''\n        image:\n          src: >-\n            https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic4.png\n        fullscreen: true\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          #### Detailed (scale 0.7+) — HTML with full interactivity\n\n\n          Here blocks turn into full-fledged UI components: with control\n          buttons, parameter fields, progress bars, selects. You can use any\n          HTML/CSS capabilities and plug in UI libraries. In this mode the\n          viewport typically contains no more than 20–50 blocks, which is\n          comfortable for detailed work.\n      - type: blog-media-block\n        column: left\n        resetPaddings: true\n        text: ''\n        image:\n          src: >-\n            https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic5.png\n        fullscreen: true\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          **What if we use FPS to choose the level of detail?**\n\n\n          We had approaches to selecting the level of detail based on FPS. But\n          it turned out that this approach creates instability—when performance\n          increases, the system switches to a more detailed mode, which lowers\n          FPS and can cause switching back—and so on in a loop.\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          ## How we arrived at the solution{#solution}\n\n\n          Okay, LOD is cool. But implementation requires Canvas for performance,\n          and that’s a new headache. Drawing on Canvas isn’t very hard—problems\n          appear when you need interactivity.\n\n\n          ### Problem: how do we understand where the user clicked?\n\n\n          In HTML it’s simple: click a button—you immediately get an event on\n          that element. With Canvas it’s harder: you click the canvas—and then\n          what? You have to figure out yourself which element the user clicked.\n\n\n          Basically there are three approaches:\n\n            * Pixel Testing (color picking),\n            * Geometric approach (simple iteration over all elements),\n            * Spatial Indexing (spatial index).\n\n          #### Pixel Testing (color picking)\n\n\n          The idea is simple: create a second invisible canvas, copy the scene\n          there, but fill each element with a unique color that will be treated\n          as the object ID. On click, read the pixel color under the mouse\n          pointer via getImageData and thus get the element ID.\n\n\n          #|\n\n          ||**Pros**|**Cons**||\n\n          ||* Implemented in a couple dozen lines\n\n\n          * Doesn’t require additional data structures|* Canvas anti-aliasing\n          blends colors—clicking on a shape boundary can produce an “invalid” ID\n\n\n          * Disabling anti-aliasing in 2D Canvas is not available\n\n\n          * A second canvas duplicates memory and doubles the render pass||\n\n          |#\n\n\n          For small scenes the method is fine, but with 10,000+ elements the\n          error rate becomes unacceptable—so we set Pixel Testing aside.\n\n\n          #### Geometric approach (simple iteration over all elements)\n\n\n          The idea is simple: iterate over all elements and check whether the\n          click point lies inside the element.\n\n\n          #|\n\n          ||**Pros**|**Cons**||\n\n          ||* Implemented in a couple dozen lines\n\n\n          * Doesn’t require additional data structures|* Very slow with a large\n          number of elements\n\n\n          * Not suitable for large scenes||\n\n          |#\n\n\n          #### Spatial Indexing\n\n\n          An evolution of the geometric approach. In the geometric approach we\n          hit the number of elements. Spatial indexing algorithms try to group\n          nearby elements in some way, mostly using trees, which makes it\n          possible to reduce complexity to log n.\n\n\n          There are quite a lot of spatial indexing algorithms; we chose the\n          R-Tree data structure via the\n          [rbush](https://github.com/mourner/rbush){target=\"_blank\"} library.\n\n\n          R-Tree is, as the name suggests, a tree where each object is placed\n          into a minimum bounding rectangle (MBR), and then those rectangles are\n          grouped into larger rectangles. This produces a tree where each\n          rectangle contains other rectangles.\n      - type: blog-media-block\n        column: left\n        resetPaddings: true\n        text: >-\n          Image from Wikipedia\n          [R‑tree](https://en.wikipedia.org/wiki/R-tree){target=\"_blank\"}\n        image:\n          src: >-\n            https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic6.png\n        fullscreen: true\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          To search in an RTree, we need to descend the tree (into the\n          rectangles) until we reach a specific element. The path is chosen by\n          checking intersection of the search rectangle with MBRs. All branches\n          whose bounding boxes don’t even touch the search rectangle are\n          discarded immediately—that’s why traversal depth is usually limited to\n          3–5 levels, and the search itself takes microseconds even with tens of\n          thousands of elements.\n\n\n          This option works, although slower (O(log n) in the best case and O(n)\n          in the worst) than pixel testing, but it is more accurate and less\n          demanding on memory.\n\n\n          #### Event model\n\n\n          Based on the RTree, we can now build our event model. When the user\n          clicks, a hit-test procedure runs: we form a 1×1 pixel rectangle at\n          the cursor coordinates and search for its intersections in the R-Tree.\n          Having obtained the element that this rectangle hits, we delegate the\n          event to that element. If the element did not stop the event, then it\n          is passed to its parent, and so on up to the root. The behavior of\n          this model resembles the familiar event model in the browser. Events\n          can be intercepted, prevented, or propagation can be stopped.\n\n\n          As I mentioned, during hit-testing we form a 1×1 pixel rectangle,\n          which means we can form a rectangle of any size. And this will help us\n          implement another very important optimization—Spatial Culling.\n\n\n          ### Spatial Culling\n\n\n          Spatial Culling is a rendering optimization technique aimed at not\n          drawing what is not visible. For example, not drawing objects that are\n          outside the camera space or that are occluded by other scene elements.\n          Since our graph is drawn in 2D space, it is sufficient for us to not\n          draw only those objects that are outside the camera’s visible area\n          (viewport).\n\n\n          How it works:\n\n            * on each camera pan or zoom we form a rectangle equal to the current viewport;\n            * search for its intersections in the R-Tree;\n            * the result is a list of elements that are actually visible;\n            * we render only them; everything else is skipped.\n\n          This technique makes performance almost independent of the total\n          number of elements: if 40 blocks fit in the frame, the library will\n          draw exactly 40, not tens of thousands hidden beyond the screen. At\n          far zoom levels, many elements fall into the viewport, so we draw\n          lightweight Canvas primitives; as the camera zooms in, the number of\n          elements decreases and freed resources allow switching to HTML mode\n          with full detail.\n\n\n          Putting it all together, we get a simple scheme:\n\n            * Canvas is responsible for speed,\n            * HTML—for interactivity,\n            * R-Tree and Spatial Culling seamlessly combine them into a single system, allowing us to quickly understand which elements can be drawn on the HTML layer.\n\n          While the camera moves, the small viewport asks the R-Tree only for\n          those objects that are actually in the frame. This approach allows us\n          to draw truly large graphs, or at least have a performance reserve\n          until the user narrows the viewport.\n\n\n          So at its core the library contains:\n\n            * a Canvas mode with simple primitives;\n            * an HTML mode with full detail;\n            * R-Tree and Spatial Culling for performance optimization;\n            * a familiar event model.\n\n          But that’s not enough for production: we need the ability to extend\n          the library and customize it to our needs.\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          ## Customization{#customization}\n\n\n          The library offers two complementary ways to extend and change\n          behavior:\n\n            * Overriding base components. Change the logic of standard Block, Anchor, Connection.\n            * Extending via layers (Layers). Add fundamentally new functionality above/below the existing scene.\n\n          ### Overriding components\n\n\n          When you need to modify the appearance or behavior of existing\n          elements, inherit from the base class and override key methods. Then\n          register the component under your own name.\n\n\n          #### Block customization\n\n\n          For example, if you need to create a graph with progress bars on\n          blocks—say, to display task execution status in a pipeline—you can\n          easily customize the standard blocks:\n\n\n          ```json\n\n          import { CanvasBlock } from \"@gravity‑ui/graph\";\n\n\n          class ProgressBlock extends CanvasBlock {\n            // Base block shape with rounded corners\n            public override renderBody(ctx: CanvasRenderingContext2D): void {\n              ctx.fillStyle = \"#ddd\";\n              ctx.beginPath();\n              ctx.roundRect(this.state.x, this.state.y, this.state.width, this.state.height, 12);\n              ctx.fill();\n              ctx.closePath();\n            }\n\n            public renderSchematicView(ctx: CanvasRenderingContext2D): void {\n              const progress = this.state.meta?.progress || 0;\n\n              // Draw the block base\n              this.renderBody(ctx);\n\n              // Progress bar with color indication\n              const progressWidth = (this.state.width - 20) * (progress / 100);\n              ctx.fillStyle = progress < 50 ? \"#ff6b6b\" : progress < 80 ? \"#feca57\" : \"#48cae4\";\n              ctx.fillRect(this.state.x + 10, this.state.y + this.state.height - 15, progressWidth, 8);\n\n              // Progress bar border\n              ctx.strokeStyle = \"#ddd\";\n              ctx.lineWidth = 1;\n              ctx.strokeRect(this.state.x + 10, this.state.y + this.state.height - 15, this.state.width - 20, 8);\n\n              // Text with percentages and name\n              ctx.fillStyle = \"#2d3436\";\n              ctx.font = \"12px Arial\";\n              ctx.textAlign = \"center\";\n              ctx.fillText(`${Math.round(progress)}%`, this.state.x + this.state.width / 2, this.state.y + 20);\n              ctx.fillText(this.state.name, this.state.x + this.state.width / 2, this.state.y + 40);\n            }\n          }\n\n          ```\n\n\n          #### Connection customization\n\n\n          Similarly, if you need to change the behavior and appearance of\n          connections—for example, to show data flow intensity between\n          blocks—you can create a custom connection:\n\n\n          ```json\n\n          import { BlockConnection } from \"@gravity-ui/graph\";\n\n\n          class DataFlowConnection extends BlockConnection {\n            public override style(ctx: CanvasRenderingContext2D) {\n              // Get flow data from the connected blocks\n              const sourceBlock = this.sourceBlock;\n              const targetBlock = this.targetBlock;\n\n              const sourceProgress = sourceBlock?.state.meta?.progress || 0;\n              const targetProgress = targetBlock?.state.meta?.progress || 0;\n\n              // Compute flow intensity based on block progress\n              const flowRate = Math.min(sourceProgress, targetProgress);\n              const isActive = flowRate > 10; // Flow is active when progress > 10%\n\n              if (isActive) {\n                // Active flow -- thick green line\n                ctx.strokeStyle = \"#00b894\";\n                ctx.lineWidth = Math.max(2, Math.min(6, flowRate / 20));\n              } else {\n                // Inactive flow -- dashed gray line\n                ctx.strokeStyle = \"#ddd\";\n                ctx.lineWidth = this.context.camera.getCameraScale();\n                ctx.setLineDash([5, 5]);\n              }\n\n              return { type: \"stroke\" };\n            }\n          }\n\n          ```\n\n\n          #### Using custom components\n\n\n          Register the created components in the graph settings:\n\n\n          ```json\n\n          const customGraph = new Graph({\n            blocks: [\n              {\n                id: \"task1\",\n                is: \"progress\",\n                x: 100,\n                y: 100,\n                width: 200,\n                height: 80,\n                name: \"Data Processing\",\n                meta: { progress: 75 },\n              },\n              {\n                id: \"task2\",\n                is: \"progress\",\n                x: 400,\n                y: 100,\n                width: 200,\n                height: 80,\n                name: \"Analysis\",\n                meta: { progress: 30 },\n              },\n              {\n                id: \"task3\",\n                is: \"progress\",\n                x: 700,\n                y: 100,\n                width: 200,\n                height: 80,\n                name: \"Output\",\n                meta: { progress: 5 },\n              },\n            ],\n            connections: [\n              { sourceBlockId: \"task1\", targetBlockId: \"task2\" },\n              { sourceBlockId: \"task2\", targetBlockId: \"task3\" },\n            ],\n            settings: {\n              // Register custom blocks\n              blockComponents: {\n                'progress': ProgressBlock,\n              },\n              // Register a custom connection for all links\n              connection: DataFlowConnection,\n              useBezierConnections: true,\n            },\n          });\n\n\n          customGraph.setEntities({\n            blocks: [\n              {\n              is: 'progress',\n              id: '1',\n              name: \"progress block',\n              x: 10, \n              y: 10, \n              width: 10, \n              height: 10,\n              anchors: [],\n              selected: false,\n              }\n            ]\n          })\n\n\n          customGraph.start();\n\n          ```\n\n\n          #### Result\n\n\n          The result is a graph where:\n\n            * blocks show current progress with color indication;\n            * connections visualize data flow: active flows are green and thick, inactive ones are gray and dashed;\n            * when zooming, blocks automatically switch to HTML mode with full interactivity.\n\n          ### Extending with layers\n\n\n          Layers are additional Canvas or HTML elements that are inserted into\n          the graph “space”. Essentially, each layer is a separate rendering\n          channel that can contain its own canvas for fast graphics or an HTML\n          container for complex interactive elements.\n\n\n          By the way, this is exactly how React integration works in our\n          library: React components are rendered into the HTML layer via a React\n          Portal.\n\n\n          #### Layer architecture\n\n\n          Layers are another key solution to the Canvas vs HTML dilemma. Layers\n          synchronize the positions of Canvas and HTML elements, ensuring they\n          overlay correctly. This makes it possible to seamlessly switch between\n          Canvas and HTML while staying in a single coordinate space. The graph\n          consists of independent layers stacked on top of each other:\n      - type: blog-media-block\n        column: left\n        resetPaddings: true\n        text: ''\n        image:\n          src: >-\n            https://storage.yandexcloud.net/gravity-landing-static/blog/canvas-vs-html/pic7.png\n        fullscreen: true\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          Layers can work in two coordinate systems:\n\n            * Attached to the graph (`transformByCameraPosition: true`):\n\n              * elements move together with the camera,\n              * blocks, connections, graph elements.\n\n            * Fixed on the screen (`transformByCameraPosition: false`):\n\n              * stay in place when panning,\n              * toolbars, legends, UI controls.\n\n          #### How React integration works\n\n\n          A layer with React integration is quite illustrative for demonstrating\n          what layers are. First, let’s look at a component that highlights the\n          list of blocks that are in the camera’s visible area. To do this, we\n          need to subscribe to camera changes and after each change check the\n          intersection of the camera viewport with the elements’ hitboxes.\n\n\n          ```json\n\n          import { Graph } from \"@gravity-ui/graph\";\n\n\n          const BlocksList = ({ graph, renderBlock }: { graph: Graph,\n          renderBlock: (graph: Graph, block: TBlock) => React.JSX.Element }) =>\n          {\n            const [blocks, setBlocks] = useState([]);\n\n            const updateVisibleList = useCallback(() => {\n              const cameraState = graph.cameraService.getCameraState();\n              const CAMERA_VIEWPORT_TRESHOLD = 0.5;\n              const x = -cameraState.relativeX - cameraState.relativeWidth * CAMERA_VIEWPORT_TRESHOLD;\n              const y = -cameraState.relativeY - cameraState.relativeHeight * CAMERA_VIEWPORT_TRESHOLD;\n              const width = -cameraState.relativeX + cameraState.relativeWidth * (1 + CAMERA_VIEWPORT_TRESHOLD) - x;\n              const height = -cameraState.relativeY + cameraState.relativeHeight * (1 + CAMERA_VIEWPORT_TRESHOLD) - y;\n              \n              const blocks = graph\n                .getElementsOverRect(\n                  {\n                    x,\n                    y,\n                    width,\n                    height,\n                  }, // defines the area in which the list of blocks will be searched\n                  [CanvasBlock] // defines the element types that will be searched in the camera viewport\n                ).map((component) => component.connectedState); // Get the list of block models\n\n                setBlocks(blocks);\n            });\n\n              useGraphEvent(graph, \"camera-change\", ({ scale }) => {\n                if (scale >= 0.7) {\n                  // If the scale is greater than 0.7, then update the list of blocks\n                  updateVisibleList()\n                  return;\n                }\n                setBlocks([]);\n              });\n\n              return blocks.map(block => <React.Fragment key={block.id}>{renderBlock(graphObject, block)}</React.Fragment>)\n          }\n\n          ```\n\n\n          Now let’s look at the description of the layer itself that will use\n          this component.\n\n\n          ```json\n\n          import { Layer } from '@gravity-ui/graph';\n\n\n          class ReactLayer extends Layer {\n            constructor(props: TReactLayerProps) {\n              super({\n                html: {\n                  zIndex: 3, // bring the layer above the other layers\n                  classNames: [\"no-user-select\"], // add a class to disable text selection\n                  transformByCameraPosition: true, // layer is attached to the camera - now the layer will move together with the camera\n                },\n                ...props,\n              });\n            }\n\n            public renderPortal(renderBlock: <T extends TBlock>(block: T) => React.JSX.Element) {\n              if (!this.getHTML()) {\n                return null;\n              }\n\n              const htmlLayer = this.getHTML() as HTMLDivElement;\n\n              return createPortal(\n                React.createElement(BlocksList, {\n                  graph: this.context.graph,\n                  renderBlock: renderBlock,\n                }),\n                htmlLayer,\n              );\n            }\n          }\n\n          ```\n\n\n          Now we can use this layer in our application.\n\n\n          ```json\n\n          import { Flex } from \"@gravity-ui/uikit\";\n\n\n          const graph = useMemo(() => new Graph());\n\n          const containerRef = useRef<HTMLDivElement>();\n\n\n          useEffect(() => {\n              if (containerRef.current) {\n                graph.attach(containerRef.current);\n              }\n\n              return () => {\n                graph.detach();\n              };\n            }, [graph, containerRef]);\n\n\n          const reactLayer = useLayer(graph, ReactLayer, {});\n\n\n          const renderBlock = useCallback((graph, block) => <Block graph={graph}\n          block={block}>{block.name}</Block>)\n\n            return (\n              <div>\n                <div style={{ position: \"absolute\", overflow: \"hidden\", width: \"100%\", height: \"100%\" }} ref={containerRef}>\n                  {graph && reactLayer && reactLayer.renderPortal(renderBlock)}\n                </div>\n              </div>\n            );\n          ```\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          Overall, everything is quite simple. Nothing described above needs to\n          be written by yourself—everything is already implemented and ready to\n          use.\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          ## Our graph library: what the benefits are and how to use\n          it{#library}\n\n\n          When we started working on the library, the main question was: how do\n          we make it so that a developer doesn’t have to choose between\n          performance and development convenience? The answer turned out to be\n          automating that choice.\n\n\n          ### Benefits\n\n\n          #### Performance + convenience\n\n\n          [@gravity‑ui/graph](https://github.com/gravity-ui/graph){target=\"_blank\"}\n          automatically switches between Canvas and HTML depending on the scale.\n          This means you get:\n\n            * Stable 60 FPS on graphs with thousands of elements.\n            * The ability to use full-fledged HTML components with rich interactivity when viewing in detail.\n            * A single event model regardless of rendering method—click, mouseenter work the same on Canvas and in HTML.\n\n          #### Compatibility with UI libraries\n\n\n          One of the main advantages is compatibility with any UI libraries. If\n          your team uses:\n\n            * Gravity UI,\n            * Material‑UI,\n            * Ant Design,\n            * custom components.\n\n          …then you don’t need to give them up! When you zoom in, the graph\n          automatically switches to HTML mode, where familiar `Button`,\n          `Select`, `DatePicker` in the color theme you need work exactly the\n          same as in a regular React app.\n\n\n          #### Framework agnostic\n\n\n          Although we implemented the basic HTML renderer using React, we tried\n          to develop the library so that it remains framework-agnostic. This\n          means that if necessary, you can fairly easily implement a layer\n          integrating your favorite framework.\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: \"\\n## Are there any alternatives?{#analogs}\\n\\nThere are quite a lot of solutions on the market for graph rendering, from paid solutions like [yFiles](https://yfiles.dev/){target=\\\"_blank\\\"}, [JointJS](https://github.com/clientIO/joint){target=\\\"_blank\\\"}, to open-source solutions [Foblex Flow](https://github.com/Foblex/f-flow){target=\\\"_blank\\\"}, [baklavajs](https://github.com/newcat/baklavajs){target=\\\"_blank\\\"}, [jsPlumb](https://github.com/jsplumb/community-edition){target=\\\"_blank\\\"}. But for comparison we consider [@antv/g6](https://github.com/antvis/G6){target=\\\"_blank\\\"} and [React Flow](https://github.com/xyflow/xyflow){target=\\\"_blank\\\"} as the most popular tools. Each of them has its own features.\\n\\nReact Flow is a good library tailored for building node-based interfaces. It has very extensive capabilities, but due to using svg and html its performance is rather modest. The library is good when you’re confident graphs won’t exceed 100–200 blocks.\\n\\nIn turn, @antv/g6 has a ton of features; it supports Canvas and in particular WebGL. Comparing @antv/g6 and @gravity‑ui/graph directly is probably not quite correct: their team is more focused on building graphs and charts—but node-based UI is also supported. So antv/g6 is suitable if you care not only about a node-based interface but also about drawing charts.\\n\\nAlthough @antv/g6 can do both canvas/webgl and html/svg, you’ll have to manage the switching rules manually, and you need to do it correctly. Performance-wise it is much faster than React Flow, but there are still questions about the library. While WebGL support is claimed, if you look at their [stress test](https://g6.antv.antgroup.com/en/examples/performance/massive-data#60000){target=\\\"_blank\\\"}, it’s noticeable that on 60k nodes the library can’t deliver dynamics—on a MacBook M3 rendering a single frame took 4 seconds. For comparison, our [stress test](https://preview.gravity-ui.com/graph/?path=/story/stories-main-grapheditor--graph-stress-test){target=\\\"_blank\\\"} on 111k nodes and 109k connections on the same Macbook M3: rendering the entire graph scene takes ~60ms, which yields ~15–20 FPS. That’s not very much, but with Spatial Culling there is an option to limit the viewport and thus improve responsiveness. Although the maintainers [stated](https://github.com/antvis/G6/issues/1597){target=\\\"_blank\\\"} they want to achieve rendering 100k nodes at 30 FPS, apparently they have not managed to do so yet.\\n\\nAnother point where @gravity‑ui/graph wins is bundle size.\\n\\n#|\\n|||Bundle size Minified|Bundle size Minified + Gzipped||\\n||@antv/g6 [bundlephobia](https://bundlephobia.com/package/@antv/g6@5.0.49){target=\\\"_blank\\\"}|1.1 MB|324.5\\_kB||\\n||react flow [bundlephobia](https://bundlephobia.com/package/@xyflow/react@12.8.1){target=\\\"_blank\\\"}|181.2\\_kB|56.4\\_kB||\\n||@gravity-ui/graph [bundlephobia](https://bundlephobia.com/package/@gravity-ui/graph){target=\\\"_blank\\\"}|2.2\\_kB|672\\_B||\\n|#\\n\\nAlthough both libraries are quite strong in terms of performance or integration convenience, @gravity‑ui/graph has a number of advantages—it can provide performance on truly large graphs while preserving UI/UX for the user and simplifying development.\\n\"\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          ## Plans for the future{#future}\n\n\n          The library already has sufficient performance headroom for most\n          tasks, so in the near future we will focus more on developing the\n          ecosystem around the library—building layers (plugins), integrations\n          for other libraries and frameworks (Angular/Vue/Svelte, …etc), adding\n          support for touch devices, adaptation for mobile browsers, and\n          generally improving UX/DX.\n      - type: blog-yfm-block\n        column: left\n        resetPaddings: true\n        text: >\n\n          ## Try it and join in{#try}\n\n\n          In the\n          [repository](https://github.com/gravity-ui/graph){target=\"_blank\"}\n          you’ll find a fully working library:\n\n            * Canvas + R-Tree core (≈ 30K lines of code),\n            * React integration,\n            * Storybook with examples.\n\n          You can install the library in one line:\n\n\n          `npm install @gravity-ui/graph`\n\n\n          --------------\n\n\n          For quite a long time, the library that is now called\n          @gravity‑ui/graph was an internal tool inside Nirvana, and the chosen\n          approach has proven itself well. Now we want to share our work and\n          help developers outside draw their graphs more easily, faster, and\n          more efficiently.\n\n\n          We want to standardize approaches to displaying complex graphs in the\n          open-source community—too many teams reinvent the wheel or struggle\n          with unsuitable tools.\n\n\n          That’s why it’s very important for us to collect your\n          feedback—different projects bring different edge cases that help\n          evolve the library. This will help us refine the library and grow the\n          Gravity UI ecosystem faster.\n  - type: blog-layout-block\n    resetPaddings: true\n    fullWidth: false\n    children:\n      - type: blog-meta-block\n        column: left\n        resetPaddings: true\n  - type: blog-suggest-block\n    resetPaddings: true\n",
    "title": "",
    "noIndex": false,
    "shareTitle": null,
    "shareDescription": null,
    "shareImage": "https://storage.yandexcloud.net/gravity-landing-static/blog/blog-cover-bg.png",
    "pageLocaleId": null,
    "author": "timofeyevvv",
    "metaDescription": null,
    "keywords": [],
    "shareGenTitle": null,
    "canonicalLink": null,
    "sharingType": "semi-full",
    "sharingTheme": "dark",
    "comment": "sharing pic",
    "shareImageUrl": "https://storage.cloud-preprod.yandex.net/ui-api-ru-preprod-stable-share-generator-screenshots/cache/b155df2ab692d6e154ff809a7d91b9ad4789de53.png",
    "pageRegionId": 76,
    "summary": null,
    "versionId": 216,
    "service": null,
    "solution": null,
    "locales": [
      {
        "id": 75,
        "pageId": 71,
        "locale": "ru",
        "createdAt": "2026-01-15T11:26:48.440Z",
        "updatedAt": "2026-01-15T11:26:48.519Z",
        "publishedVersionId": null,
        "lastVersionId": 195
      },
      {
        "id": 76,
        "pageId": 71,
        "locale": "en",
        "createdAt": "2026-01-15T11:26:48.532Z",
        "updatedAt": "2026-01-15T11:26:48.609Z",
        "publishedVersionId": null,
        "lastVersionId": 196
      }
    ],
    "pageRegions": [
      {
        "regionCode": "ru-ru",
        "publishedVersionId": 199
      },
      {
        "regionCode": "en",
        "publishedVersionId": 216
      }
    ],
    "searchCategory": {
      "id": 7,
      "slug": "blog",
      "title": "Blog",
      "url": "/blog"
    },
    "voiceovers": []
  }
  